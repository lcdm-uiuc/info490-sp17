{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1206a454e6c628e914177ddb325af6a9",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "If you are not using the `Assignments` tab on the course JupyterHub server to read this notebook, read [Activating the assignments tab](https://github.com/lcdm-uiuc/info490-sp17/blob/master/help/act_assign_tab.md).\n",
    "\n",
    "A few things you should keep in mind when working on assignments:\n",
    "\n",
    "1. Make sure you fill in any place that says `YOUR CODE HERE`. Do **not** write your answer in anywhere else other than where it says `YOUR CODE HERE`. Anything you write anywhere else will be removed or overwritten by the autograder.\n",
    "\n",
    "2. Before you submit your assignment, make sure everything runs as expected. Go to menubar, select _Kernel_, and restart the kernel and run all cells (_Restart & Run all_).\n",
    "\n",
    "3. Do not change the title (i.e. file name) of this notebook.\n",
    "\n",
    "4. Make sure that you save your work (in the menubar, select _File_  â†’ _Save and CheckPoint_)\n",
    "\n",
    "5. You are allowed to submit an assignment multiple times, but only the most recent submission will be graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3f54b0887077eb7d28d38f17537f2a46",
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Problem 12.1. Intro to Hadoop.\n",
    "In this problem set, you will be doing simple exercises using Hadoop.  Before you start, however, you should be aware of the following: __you MUST delete YOUR CODE HERE in order for your code to work (comments beginning with # are NOT kosher for command-line statements!!)__. \n",
    "\n",
    "When you comment your code for this assignment, please make the comments either above or below any command-line statements (lines starting with !)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "6b6a2dbd8e3c9d281b7da5de9ff831ce",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nose.tools import assert_equal, assert_true, assert_is_instance\n",
    "from numpy.testing import assert_array_almost_equal, assert_almost_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "60e9bff332c396105192ea16d8f129e0",
     "grade": false,
     "grade_id": "markdown_1",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "First, we make sure that namenodes and datanodes are stopped, formatted and started, and make sure to get rid of any spurious files that might gunk up our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!$HADOOP_PREFIX/sbin/stop-dfs.sh\n",
    "!$HADOOP_PREFIX/sbin/stop-yarn.sh\n",
    "!rm -rf /tmp/*\n",
    "!echo \"Y\" | $HADOOP_PREFIX/bin/hdfs namenode -format 2> /dev/null\n",
    "!$HADOOP_PREFIX/etc/hadoop/hadoop-env.sh\n",
    "!$HADOOP_PREFIX/sbin/start-dfs.sh\n",
    "!$HADOOP_PREFIX/sbin/start-yarn.sh\n",
    "!$HADOOP_PREFIX/bin/hdfs dfsadmin -safemode leave\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -mkdir -p /user/$NB_USER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "469224e87b2040496fdf04b21e2e7e39",
     "grade": false,
     "grade_id": "linearreg",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Part 1: Exploring files and the system\n",
    "First, let's start out by listing the contents of the directory `/user/` in HDFS.  When you do this, you will pipe the output into a file called temp1.txt so that you may pass the assertion tests below (the easiest way to do this piping is with the `>temp1.txt` statement after your command-line statement). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "0a35c49169cdea4ac50c0cb1c61051d0",
     "grade": false,
     "grade_id": "folder_list",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "!YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "e4650249ebbb3f684003b7c07d26dad9",
     "grade": true,
     "grade_id": "folder_list1",
     "locked": true,
     "points": 8,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "res1 = !cat temp1.txt\n",
    "\n",
    "assert_is_instance(res1, list)\n",
    "assert_is_instance(res1[0], str)\n",
    "assert_is_instance(res1[1], str)\n",
    "assert_true(res1[1], \"Found 1 items\")\n",
    "assert_true(res1[1][:40], \"drwxr-xr-x   - data_scientist supergroup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b7dc34fca00e2536d98cdc015a6ca113",
     "grade": false,
     "grade_id": "free_space",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Part 2 Free space: \n",
    "Now, let's issue a Hadoop command that allows us to see the free space available to us, making sure to make it human readable. Like before, you will pipe the output into a file called temp2.txt so that you may pass the assertion tests below (this piping can be done by putting `>temp2.txt` after your command-line statement). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "7d257d84da1613767a8e457f836de578",
     "grade": false,
     "grade_id": "free_space_test",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "!YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "f0d05bfeb8d21609d81980257ba044ed",
     "grade": true,
     "grade_id": "folder_list2",
     "locked": true,
     "points": 8,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "res2 = !cat temp2.txt\n",
    "assert_is_instance(res2, list)\n",
    "assert_is_instance(res2[0], str)\n",
    "assert_is_instance(res2[1], str)\n",
    "assert_true(len(res2), 2)\n",
    "assert_true(res2[0], \"Filesystem                                             Size  Used  Available  Use%\")\n",
    "assert_true(res2[1][:46], \"hdfs://info490rb.studentspace.cs.illinois.edu:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Version\n",
    "Next, let's get the version information of Hadoop that we are running, making sure to pipe the output into the vers.txt file provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "bbd83305b9236a04aff04f5f873b404e",
     "grade": false,
     "grade_id": "version",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "!YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "a28cebc0348b3c8c33ff719899ac8bb5",
     "grade": true,
     "grade_id": "version_test",
     "locked": true,
     "points": 8,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "vers = !cat vers.txt\n",
    "assert_true(all(isinstance(w, str) for w in vers))\n",
    "assert_true(vers[0], 'Hadoop 2.7.2')\n",
    "assert_true(vers[3], 'Compiled with protoc 2.5.0')\n",
    "assert_true(len(vers), 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up files\n",
    "Run this cell before restarting and rerunning your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm temp1.txt\n",
    "!rm temp2.txt\n",
    "!rm vers.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e4de9af7ef2166a8c371668ea4c478bc",
     "grade": false,
     "grade_id": "getting_ready",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## New directory for Hadoop\n",
    "Here, I'm creating a new directory for Hadoop so that we are ready for the next two coding parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "DIR=$HOME/hadoop_assign\n",
    "\n",
    "# Delete if exists\n",
    "if [ -d \"$DIR\" ]; then\n",
    "    rm -rf \"$DIR\"\n",
    "fi\n",
    "\n",
    "# Now make the directory\n",
    "mkdir \"$DIR\"\n",
    "\n",
    "ls -la $DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Copying a book into a directory\n",
    "For these final two coding sections, we will be dealing with the script for Monty Python and the Holy Grail.\n",
    "\n",
    "For this section, you must copy the file grail.txt from here:  \n",
    "\n",
    "`/home/data_scientist/data/nltk_data/corpora/webtext/` \n",
    "\n",
    "into your hadoop_assign directory that you just created in above your $HOME directory. Please use `cp` and do not use Hadoop commands here or else you might fail the assertion tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "26eba3858f5555463e45ca114d67e28d",
     "grade": false,
     "grade_id": "copy_grail",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "!YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "18f51efe2a2f63c8c001de19998010a3",
     "grade": true,
     "grade_id": "grail_test",
     "locked": true,
     "points": 8,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "!ls $HOME/hadoop_assign >copy.txt\n",
    "copy = !cat copy.txt\n",
    "assert_is_instance(copy[0], str)\n",
    "assert_is_instance(copy, list)\n",
    "assert_true(len(copy), 1)\n",
    "assert_true(copy[0], 'grail.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Making a new directory and copying a book in Hadoop\n",
    "Finally, we will do two things: we will create a new directory called `grail/in` using Hadoop, and then we will put grail.txt (located in `$HOME/hadoop_assign/`) into `grail/in`, once again using Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "3eb255f13c5c108cbd5c23fda162c5dc",
     "grade": false,
     "grade_id": "make_copy",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "!YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "ad4ffc7c10e6c1f091e5812a1fbed864",
     "grade": true,
     "grade_id": "make_copy_test",
     "locked": true,
     "points": 8,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "had_grail = !$HADOOP_PREFIX/bin/hdfs dfs -count -h grail/in/*\n",
    "had_grail = had_grail[0].split()\n",
    "assert_is_instance(had_grail, list)\n",
    "assert_true(all(isinstance(w, str) for w in had_grail))\n",
    "assert_true(had_grail, ['0', '1', '63.5', 'K', 'grail/in/grail.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up \n",
    "Please run this before you restart and run your assignment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm copy.txt\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -rm -r -f grail\n",
    "!rm -rf $HOME/hadoop_assign\n",
    "!$HADOOP_PREFIX/sbin/stop-dfs.sh\n",
    "!$HADOOP_PREFIX/sbin/stop-yarn.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
