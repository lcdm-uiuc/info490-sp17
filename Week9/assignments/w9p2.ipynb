{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1891bdcd87c6c960df75b6e90b02b234",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "If you are not using the `Assignments` tab on the course JupyterHub server to read this notebook, read [Activating the assignments tab](https://github.com/lcdm-uiuc/info490-sp17/blob/master/help/act_assign_tab.md).\n",
    "\n",
    "A few things you should keep in mind when working on assignments:\n",
    "\n",
    "1. Make sure you fill in any place that says `YOUR CODE HERE`. Do **not** write your answer in anywhere else other than where it says `YOUR CODE HERE`. Anything you write anywhere else will be removed or overwritten by the autograder.\n",
    "\n",
    "2. Before you submit your assignment, make sure everything runs as expected. Go to menubar, select _Kernel_, and restart the kernel and run all cells (_Restart & Run all_).\n",
    "\n",
    "3. Do not change the title (i.e. file name) of this notebook.\n",
    "\n",
    "4. Make sure that you save your work (in the menubar, select _File_ â†’ _Save and CheckPoint_)\n",
    "\n",
    "5. You are allowed to submit an assignment multiple times, but only the most recent submission will be graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cff737ab04ed5f48f39a33ba2b9ead9e",
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Problem 9.2. NLP: Topic Modeling.\n",
    "\n",
    "In this problem, we explore the concept of topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "55ab0d334275e9470adb6d10466fc36d",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cross_validation import check_random_state\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "from nose.tools import assert_equal, assert_is_instance, assert_true, assert_almost_equal\n",
    "from numpy.testing import assert_array_equal, assert_array_almost_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "49943f029f58697ca90647268f1b2467",
     "grade": false,
     "grade_id": "markdown_1",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We will be using the reuters data (nltk.corpus.reuters). The X_train, X_test, y_train, and y_test have already been preprocessed and saved in JSON format, for convenience in the `/home/data_scientist/data/misc` directory. Using the code below, we will fetch them for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "a9dedd22f27d222a6c0bf31912aaaab4",
     "grade": false,
     "grade_id": "reuters",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_reuters(name):\n",
    "    fpath = '/home/data_scientist/data/misc/reuters_{}.json'.format(name)\n",
    "    with open(fpath) as f:\n",
    "        reuters = json.load(f)\n",
    "    return reuters\n",
    "\n",
    "X_train, X_test, y_train, y_test = map(load_reuters, ['X_train', 'X_test', 'y_train', 'y_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ec746983cffee5ae55180f441ee3716f",
     "grade": false,
     "grade_id": "markdown_2",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Document term matrix\n",
    "\n",
    "- Use TfidfVectorizer to create a document term matrix for both `X_train` and `X_test`.\n",
    "- Use English stop words.\n",
    "- Use unigrams and bigrams.\n",
    "- Ignore terms that have a document frequency strictly lower than 2.\n",
    "- Build a vocabulary that only consider the top 20,000 features ordered by term frequency across the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "36a64e0bacc6868463950e6d8fd4d0ff",
     "grade": false,
     "grade_id": "get_document_term_matrix_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_document_term_matrix(train_data, test_data):\n",
    "    '''\n",
    "    Uses TfidfVectorizer to create a document term matrix for \"X_train\" and \"X_test\".\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    train_data: A list of strings\n",
    "    test_data:A list of strings\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A 3-tuple of (model, train_matrix, test_matrix).\n",
    "    model: A TfidfVectorizer instance\n",
    "    train_matrix: A scipy.csr_matrix\n",
    "    test_matrix: A scipy.csr_matrix\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    return model, train_matrix, test_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "c31f6bd9fde2905cff19bf5c31a5d144",
     "grade": false,
     "grade_id": "get_document_term_matrix_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "cv, train_data, test_data = get_document_term_matrix(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "53b6fdecf40744f94323b5d6d811a9b2",
     "grade": true,
     "grade_id": "get_document_term_matrix_test",
     "locked": true,
     "points": 5,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(cv, TfidfVectorizer)\n",
    "assert_is_instance(train_data, csr_matrix)\n",
    "assert_is_instance(test_data, csr_matrix)\n",
    "assert_equal(cv.stop_words, 'english')\n",
    "assert_equal(cv.ngram_range, (1, 2))\n",
    "assert_equal(cv.min_df, 2)\n",
    "assert_equal(cv.max_features, 20000)\n",
    "assert_equal(train_data.data.size, 588963)\n",
    "assert_array_almost_equal(\n",
    "    train_data.data[:5],\n",
    "    [0.0375267,   0.0401517,   0.03477509,  0.0474274,   0.03217005]\n",
    "    )\n",
    "assert_equal(test_data.data.size, 210403)\n",
    "assert_array_almost_equal(\n",
    "    test_data.data[:5],\n",
    "    [ 0.02399319,  0.04801429,  0.04859632,  0.0403796,   0.0403796]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0c94a4f3e3244ae3a1e134a7913a332c",
     "grade": false,
     "grade_id": "markdown_3",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Non-negative matrix factorization\n",
    "\n",
    "- Apply non-negative matrix factorization (NMF) to compute topics in `train_data`.\n",
    "- Use 60 topics.\n",
    "- Normalize the transformed data to have unit probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "7c010d77b4730bf96cd1a4d64b1fcc86",
     "grade": false,
     "grade_id": "apply_nmf_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_nmf(data, random_state):\n",
    "    '''\n",
    "    Applies non-negative matrix factorization (NMF) to compute topics.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: A csr_matrix\n",
    "    random_state: A RandomState instance for NMF\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of (nmf, transformed_data)\n",
    "    nmf: An sklearn.NMF instance\n",
    "    transformed_data: A numpy.ndarray\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    return nmf, transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "426f1943e7092db4fc3265eda44efb8e",
     "grade": false,
     "grade_id": "apply_nmf_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "nmf, td_norm = apply_nmf(train_data, random_state=check_random_state(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "096ee9f6fd26db25ef77c7368187b657",
     "grade": true,
     "grade_id": "apply_nmf_test",
     "locked": true,
     "points": 10,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(nmf, NMF)\n",
    "assert_is_instance(td_norm, np.ndarray)\n",
    "assert_equal(nmf.n_components, 60)\n",
    "assert_equal(nmf.max_iter, 200)\n",
    "assert_equal(td_norm.shape, (7769, 60))\n",
    "assert_array_almost_equal(\n",
    "    td_norm[0, :5],\n",
    "    [0. ,         0.08515023,  0.01682892,  0.,          0.02451052]\n",
    "    )\n",
    "assert_array_almost_equal(\n",
    "    td_norm[-1, -5:],\n",
    "    [  0.,          0.,          0.,         0.00342309,  0.        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "732c13b46243eb41c9197e60fd6fcf1a",
     "grade": false,
     "grade_id": "mardkwon_4",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Topic-based Classification\n",
    "\n",
    "- Train a LinearSVC classifier on the topics in the training data sample of the reuters data set.\n",
    "- Use default parameters for the LinearSVC classifier. Don't forget to set the `random_state` parameter.\n",
    "- Compute the topics, by using the previously created NMF model, for the test data and compute classifications from these topic models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "e721041a23a65c25ac1df44cce6cf46a",
     "grade": false,
     "grade_id": "classify_topics_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def classify_topics(nmf, X_train, y_train, X_test, random_state):\n",
    "    '''\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    nmf: An sklearn.NMF model.\n",
    "    X_train: A numpy array.\n",
    "    y_train: A numpy array.\n",
    "    X_test: A scipy csr_matrix.\n",
    "    random_state: A RandomState instance for LinearSVC Classifier.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of (clf, y_pred)\n",
    "    clf: A LinearSVC instance.\n",
    "    y_pred: A numpy array.\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return clf, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "950e39736150bc81546612927ba39e78",
     "grade": false,
     "grade_id": "classify_topics_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "clf, ts_preds = classify_topics(\n",
    "    nmf, nmf.transform(train_data), y_train, test_data, check_random_state(0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "184527224b84b48683d1a143d0f07be9",
     "grade": true,
     "grade_id": "classify_topics_test",
     "locked": true,
     "points": 5,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(clf, LinearSVC)\n",
    "assert_is_instance(ts_preds, np.ndarray)\n",
    "assert_equal(len(ts_preds), len(y_test))\n",
    "assert_array_equal(ts_preds[:5], ['trade', 'grain', 'crude', 'earn', 'crude'])\n",
    "assert_array_equal(ts_preds[-5:], ['acq', 'dlr', 'crude', 'grain', 'acq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7338bc8d00ad62d31633076c99264b40",
     "grade": false,
     "grade_id": "markdown_6",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Topic Modeling with Gensim\n",
    "\n",
    "- Use the gensim library to perform topic modeling of the reuters data. First transform a sparse matrix into a gensim corpus, and then construct a vocabulary dictionary. Finally, create a  Latent Dirichlet allocation (LDA) model with 20 topics for the reuters text, and return 5 most significant words for each topic.\n",
    "- You should specify three parameters in `LdaModel()`: `corpus`, `id2word`, and `num_topics`. Use default values for all other paramters. Ignore any warnings about `passes` or `iterations`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "0cf95c209cf1ec904cd96672108ebde0",
     "grade": false,
     "grade_id": "get_topics_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_topics(cv, train_data):\n",
    "    '''\n",
    "    Uses gensim to perform topic modeling.\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    cv: A TfidfVectorizer instance.\n",
    "    train_data: A scipy csr_matrix.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A list of strings (functions of the most important terms in each topic).\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "a59f5f881e6e96f699813e444c0ba77d",
     "grade": false,
     "grade_id": "get_topics_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "topics = get_topics(cv, train_data)\n",
    "\n",
    "for idx, (lst, val) in enumerate(topics):\n",
    "    print('Topic {0}'.format(idx))\n",
    "    print(35*('-'))\n",
    "    for i, z in lst:\n",
    "        print('    {0:20s}: {1:5.4f}'.format(z, i))\n",
    "    print(35*('-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "a9979dc296f99568d488105aa65a8bdb",
     "grade": true,
     "grade_id": "get_topics_test",
     "locked": true,
     "points": 10,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(topics, list)\n",
    "assert_equal(len(topics), 20)\n",
    "\n",
    "for topic, score in topics:\n",
    "    assert_is_instance(topic, list)\n",
    "    assert_is_instance(score, float)\n",
    "    assert_equal(len(topic), 5)\n",
    "    for v, k in topic:\n",
    "        assert_is_instance(k, str)\n",
    "        assert_is_instance(v, float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline of FeatureUnion and Logistic Regression\n",
    "\n",
    "- Build a pipeline by using [FeatureUnion](http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html) of  and  `LinearSVC`. \n",
    "- A FeatureUnion helps process data in parallel and can be considered pipelines themselves (check this [resource](http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html) for an overview on how FeatureUnion is used in NLP).\n",
    "- The first step of the pipeline should use the FeatureUnion with the name `features`. The first component of the FeatureUnion should contain the `CountVectorizer`, using the name `cv` , followed by the `TfidfVectorizer` using the name `tf`. \n",
    "- The second step of the pipeline should be `LinearSVC` with the name `svc`.\n",
    "- Do not use stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "2df182ee3bda6962e202552bcdb35044",
     "grade": false,
     "grade_id": "cv_tfidf_svc_pipe_ans",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def cv_tfidf_svc_pipe(X_train, y_train, X_test, random_state):\n",
    "    '''\n",
    "    Creates a document term matrix and uses SVM classifier to make document classifications.\n",
    "    Uses English stop words.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: A list of strings.\n",
    "    y_train: A list of strings.\n",
    "    X_test: A list of strings.\n",
    "    random_state: A np.random.RandomState instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of (clf, y_pred)\n",
    "    clf: A Pipeline instance.\n",
    "    y_pred: A numpy array.\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    return clf, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "5439acb7dde999c9fa0491b4d7c0cb0b",
     "grade": false,
     "grade_id": "cv_tfidf_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "clf2, y_pred2 = cv_tfidf_svc_pipe(X_train, y_train, X_test, random_state=check_random_state(0))\n",
    "score2 = accuracy_score(y_pred2, y_test)\n",
    "print(\"SVC prediction accuracy = {0:3.1f}%\".format(100.0 * score2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "356657210c9041ab4e66120d53d310a2",
     "grade": true,
     "grade_id": "cv_tfidf_svc_pipe_test",
     "locked": true,
     "points": 10,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(clf2, Pipeline)\n",
    "assert_is_instance(y_pred2, np.ndarray)\n",
    "assert_is_instance(clf2.named_steps['features'], FeatureUnion)\n",
    "assert_is_instance(clf2.named_steps['svc'], LinearSVC)\n",
    "assert_is_instance(clf2.named_steps['features'].transformer_list[0][1], CountVectorizer)\n",
    "assert_is_instance(clf2.named_steps['features'].transformer_list[1][1], TfidfVectorizer)\n",
    "assert_equal(len(y_pred2), len(y_test))\n",
    "assert_array_equal(y_pred2[:5], ['trade', 'grain', 'crude', 'corn', 'palm-oil'])\n",
    "assert_array_equal(y_pred2[-5:], ['acq', 'dlr', 'earn', 'ipi', 'gold'])\n",
    "assert_almost_equal(score2, 0.884067572044, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
